# Local-Browsing-LLM
LLM ran locally using ollama for giving up to date answers
